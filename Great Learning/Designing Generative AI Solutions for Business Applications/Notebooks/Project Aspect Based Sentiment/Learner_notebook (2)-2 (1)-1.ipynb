{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# **Project : A Case Study of InnovaTech Solutions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Business Overview:**\n",
        "\n",
        "InnovaTech has expanded its presence in the digital retail world, especially on e-commerce giants like Amazon. This strategic move has not only widened its customer base but also resulted in a large influx of customer feedback, primarily in the form of online reviews. The company's products, notably its range of laptops, have become popular choices on these platforms, leading to an abundance of valuable but underutilized customer data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Current Challenge:**\n",
        "\n",
        "InnovaTech currently analyzes customer reviews using basic sentiment analysis tools, which only provides a superficial understanding of customer opinions. In the competitive landscape of the laptop market, a more detailed and aspect-oriented analysis is crucial. Understanding specific customer sentiments on different aspects of laptops, such as user screen, technical specifications, etc, which is vital for targeted product improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Objective:**\n",
        "\n",
        "The primary goal is to conduct a comprehensive aspect-based sentiment analysis of customer reviews for InnovaTechâ€™s laptops, specifically focusing on three critical aspects: the laptop screen, keyboard, and mousepad. These components have been identified as crucial determinants of customer satisfaction and product usability. The project aims to provide nuanced insights into specific areas of customer satisfaction, dissatisfaction, and neutral feedback.The ultimate goal is to enhance overall product quality and customer experience, solidifying InnovaTech's position as a leader in the laptop market."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Data Description:**\n",
        "\n",
        "The dataset titled \"laptop_reviews.csv\" is structured to facilitate aspect-based sentiment analysis for laptop reviews. Here's a brief description of the data columns:\n",
        "\n",
        "1. id: This column contains unique identifiers for each review entry. It helps in distinguishing and referencing individual reviews\n",
        "2. text: This column includes the actual text of the laptop reviews. The reviews are likely composed of customer opinions and experiences regarding different aspects of the laptops.\n",
        "3. aspects: Contains structured information about specific aspects mentioned in each review like 'RAM', 'screen', 'keyboard', 'mousepad', and others relevant to laptop features.\n",
        "4. category: Provides an additional layer of classification (positive, negative and neutral) for the mentioned aspects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 1. Setup (2 Marks)**\n",
        "\n",
        "(A) Writing/Creating the config.json file  (2 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "!pip install openai==1.2 tiktoken datasets session-info --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Import all Python packages required to access the Azure Open AI API.\n",
        "# Import additional packages required to access datasets and create examples.\n",
        "\n",
        "from openai import AzureOpenAI\n",
        "import json\n",
        "import random\n",
        "import tiktoken\n",
        "import session_info\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ast\n",
        "\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "session_info.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Authentication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(A) Writing/Creating the config.json file (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define your configuration information\n",
        "config_data = {\n",
        "    \"AZURE_OPENAI_KEY\": \"<API_KEY>\",            #Replace it with your credentials\n",
        "    \"AZURE_OPENAI_ENDPOINT\": \"<ENDPOINT>\",      #Replace it with your credentials\n",
        "    \"AZURE_OPENAI_APIVERSION\": \"<API_VERSION>\", #Replace it with your credentials\n",
        "    \"CHATGPT_MODEL\": \"<MODEL_NAME>\"             #Replace it with your credentials\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write the configuration information into the config.json file\n",
        "with open('config.json', 'w') as config_file:\n",
        "    json.dump(config_data, config_file, indent=4)\n",
        "\n",
        "print(\"Config file created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "with open('config.json', 'r') as az_creds:\n",
        "    data = az_creds.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "creds = json.loads(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "client = AzureOpenAI(\n",
        "    azure_endpoint=creds[\"AZURE_OPENAI_ENDPOINT\"],\n",
        "    api_key=creds[\"AZURE_OPENAI_KEY\"],\n",
        "    api_version=creds[\"AZURE_OPENAI_APIVERSION\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "chat_model_id = creds[\"CHATGPT_MODEL\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def num_tokens_from_messages(messages):\n",
        "\n",
        "    \"\"\"\n",
        "    Return the number of tokens used by a list of messages.\n",
        "    Adapted from the Open AI cookbook token counter\n",
        "    \"\"\"\n",
        "\n",
        "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Each message is sandwiched with <|start|>role and <|end|>\n",
        "    # Hence, messages look like: <|start|>system or user or assistant{message}<|end|>\n",
        "\n",
        "    tokens_per_message = 3 # token1:<|start|>, token2:system(or user or assistant), token3:<|end|>\n",
        "\n",
        "    num_tokens = 0\n",
        "\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "\n",
        "    return num_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Task: Aspect-Based Sentiment Analysis (ABSA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2: Assemble Data (5 Marks)**\n",
        "\n",
        "(A) Upload and Read csv File (1 Mark)\n",
        "\n",
        "(B) Split the Dataset (2 Marks)\n",
        "\n",
        "(C) Create a dictionary of aspects by creating two indexes - \"examples_aspect_index\" and \"gold_examples_aspect_index\" (1 Mark)\n",
        "\n",
        "(D) Print the count of examples for each aspect in examples_aspect_index and gold_examples_aspect_index (1 mark)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(A) Upload and read csv file (1 Mark)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "laptop_reviews_df = \"__________\"\n",
        "# Read CSV File Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check header of dataframe to examine data structure\n",
        "laptop_reviews_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(B) Split the Dataset (2 Marks)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that the preprocessing is done, let us split the data into two segments (use split_ratio of 0.2) - one segment (80%) that gives us a pool to draw few-shot examples from and another segment (20%) that gives us a pool of gold examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "laptop_reviews_examples_df, laptop_reviews_gold_examples_df = train_test_split(\n",
        "    \"__________\", #<- the full dataset\n",
        "    \"__________\", #<- 20% random sample selected for gold examples\n",
        "    random_state=42 #<- ensures that the splits are the same for every session\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(C) Create a dictionary of aspects by creating two indexes - \"examples_aspect_index\" and \"gold_examples_aspect_index\"** (1 Mark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "examples_aspect_index = {\n",
        "    '________': [],\n",
        "    '________': [],\n",
        "    '________': []\n",
        "}\n",
        "\n",
        "gold_examples_aspect_index = {\n",
        "    '________': [],\n",
        "    '________': [],\n",
        "    '________': []\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(D) Print the count of examples for each aspect in examples_aspect_index and gold_examples_aspect_index (1 mark)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for id, category in zip(laptop_reviews_examples_df.id, laptop_reviews_examples_df.category):\n",
        "    for key in examples_aspect_index.keys():\n",
        "        if key in category:\n",
        "            examples_aspect_index[key].append(id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for examples_aspect_index\n",
        "for key in __________:                            \n",
        "    print(f\"Number of examples for aspect {key}: {len(__________[key])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for id, category in zip(laptop_reviews_gold_examples_df.id, laptop_reviews_gold_examples_df.category):\n",
        "    for key in gold_examples_aspect_index.keys():\n",
        "        if key in category:\n",
        "            gold_examples_aspect_index[key].append(id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for gold_examples_aspect_index\n",
        "for key in __________:                            \n",
        "    print(f\"Number of examples for aspect {key}: {len(__________[key])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_to_select = ['id', 'text', 'category']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gold_examples = json.loads((\n",
        "        laptop_reviews_gold_examples_df.loc[:, columns_to_select]\n",
        "                                           .sample(15, random_state=42) #<- ensures that gold examples are the same for every session\n",
        "                                           .to_json(orient='records')\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gold_examples[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3: Derive Prompt (12 Marks)**\n",
        "\n",
        "(A) Write Zero Shot System Message (3 Marks)\n",
        "\n",
        "(B) Create Zero Shot Prompt (2 Marks)\n",
        "\n",
        "(C) Write Few Shot System Message (3 Marks)\n",
        "\n",
        "(D) Create Examples For Few shot prompte (2 Marks)\n",
        "\n",
        "(E) Create Few Shot Prompt (2 Marks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_message_template = \"\"\"```{laptop_review}```\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**(A) Write Zero Shot System Message (3 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "zero_shot_system_message = \"\"\"__________\"\"\"\n",
        "# Write zero shot system message here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(B) Create Zero Shot Prompt (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "zero_shot_prompt = \"__________\"\n",
        "# Create zero shot prompt to be input ready for completion function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_tokens_from_messages(zero_shot_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**(C) Write Few Shot System Message (3 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "few_shot_system_message = \"\"\"__________\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_examples(dataset, n=4):\n",
        "\n",
        "    \"\"\"\n",
        "    Return a JSON list with n random examples of each aspect in the\n",
        "    input dataset.\n",
        "    First create a dictionary with the aspects as keys and the ids of the\n",
        "    reviews that contain this aspect as values.\n",
        "    Then take a random sample of ids from each of these lists.\n",
        "\n",
        "    Args:\n",
        "        dataset (DataFrame): DataFrame with nested ABSA annotations\n",
        "        n (int): Number of random examples selected for each aspect\n",
        "\n",
        "    Output:\n",
        "        examples (JSON): JSON list of examples\n",
        "    \"\"\"\n",
        "\n",
        "    columns_to_select = ['id', 'text', 'category']\n",
        "    example_ids = []\n",
        "\n",
        "    aspect_index = {\n",
        "        'screen': [], 'keyboard': [], 'mousepad': []\n",
        "    }\n",
        "\n",
        "    for id, category in zip(dataset.id, dataset.category):\n",
        "        for key in aspect_index.keys():\n",
        "            if key in category:\n",
        "                aspect_index[key].append(id)\n",
        "\n",
        "    for key in aspect_index:\n",
        "        example_ids.extend(np.random.choice(aspect_index[key], n).tolist())\n",
        "\n",
        "    examples = dataset.loc[dataset.id.isin(example_ids), columns_to_select]\n",
        "\n",
        "    return examples.to_json(orient='records')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(D) Create Examples For Few shot prompte (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "examples = \"__________\"\n",
        "# Create Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "json.loads(examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the examples in place, we can now assemble a few-shot prompt. Since we will be using the few-shot prompt several times during evaluation, let us write a function to create a few-shot prompt (the logic of this function is depicted below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_prompt(system_message, examples, user_message_template):\n",
        "\n",
        "    \"\"\"\n",
        "    Return a prompt message in the format expected by the Open AI API.\n",
        "    Loop through the examples and parse them as user message and assistant\n",
        "    message.\n",
        "\n",
        "    Args:\n",
        "        system_message (str): Instructions for the model to execute ABSA\n",
        "        examples (JSON): JSON list of examples representative of each aspect\n",
        "        user_message_template (str): string with a placeholder for laptop reviews\n",
        "\n",
        "    Output:\n",
        "        few_shot_prompt (List): A list of dictionaries in the Open AI prompt format\n",
        "    \"\"\"\n",
        "\n",
        "    few_shot_prompt = [{'role':'system', 'content': system_message}]\n",
        "\n",
        "    for example in json.loads(examples):\n",
        "        example_input = example['text']\n",
        "        example_absa = example['category']\n",
        "\n",
        "        few_shot_prompt.append(\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': user_message_template.format(\n",
        "                    laptop_review=example_input\n",
        "                )\n",
        "            }\n",
        "        )\n",
        "\n",
        "        few_shot_prompt.append(\n",
        "            {'role': 'assistant', 'content': f\"{example_absa}\"}\n",
        "        )\n",
        "\n",
        "    return few_shot_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(E) Create Few Shot Prompt (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "few_shot_prompt = \"__________\"\n",
        "# Create Few shot prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "few_shot_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_tokens_from_messages(few_shot_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 4: Evaluate prompts (8 Marks)**\n",
        "\n",
        "(A) Evaluate Zero Shot Prompt (2 Marks)\n",
        "\n",
        "(B) Evaluate Few Shot Prompt (2 marks)\n",
        "\n",
        "(C) Calculate Mean and Standard Deviation for Few Shot Prompt (4 Marks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have two sets of prompts that we need to evaluate using gold labels. Since the few-shot prompt depends on the sample of examples that was drawn to make up the prompt, we expect some variability in evaluation. Hence, we evaluate each prompt multiple times to get a sense of the average and the variation around the average.\n",
        "\n",
        "To reiterate, a choice on the prompt should account for variability due to the choice of the random sample. To aid repeated evaluation, we assemble an evaluation function ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def custom_parse(string):\n",
        "    result = {}\n",
        "    # Remove the outermost curly braces and split by ', \"'\n",
        "    parts = string.strip('{}').split(', \"')\n",
        "\n",
        "    for part in parts:\n",
        "        # Split each part by the first occurrence of '\": '\n",
        "        split_index = part.find('\": ')\n",
        "        if split_index == -1:\n",
        "            continue  # Skip if the format is not as expected\n",
        "\n",
        "        key = part[:split_index].strip('\"')\n",
        "        value = part[split_index+3:].strip()\n",
        "\n",
        "        # Handle the array and extract the first element\n",
        "        if value.startswith('array(['):\n",
        "            value = value[7:]  # Remove 'array(['\n",
        "            value = value.split('], dtype=object')[0]  # Get the first element\n",
        "            value = value.strip('[\"]')  # Remove quotes and brackets\n",
        "\n",
        "        result[key] = value\n",
        "\n",
        "    return result\n",
        "\n",
        "def compute_accuracy(gold_examples, model_predictions, ground_truths):\n",
        "    correct_predictions = 0\n",
        "    total_predictions = len(gold_examples)\n",
        "\n",
        "    for pred, truth in zip(model_predictions, ground_truths):\n",
        "        pred_dict = custom_parse(pred)\n",
        "        truth_dict = custom_parse(truth)\n",
        "\n",
        "        if pred_dict == truth_dict:\n",
        "            correct_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions\n",
        "    return accuracy\n",
        "\n",
        "def evaluate_prompt(prompt, gold_examples, user_message_template):\n",
        "    model_predictions, ground_truths = [], []\n",
        "\n",
        "    for example in gold_examples:\n",
        "        user_input = [{\n",
        "            'role': 'user',\n",
        "            'content': user_message_template.format(laptop_review=example['text'])\n",
        "        }]\n",
        "\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=chat_model_id,\n",
        "                messages=prompt + user_input,\n",
        "                temperature=0,\n",
        "                #max_tokens=2\n",
        "            )\n",
        "            \n",
        "            prediction = response.choices[0].message.content\n",
        "\n",
        "            #prediction = response['choices'][0]['message']['content']\n",
        "            # Convert to string representation of dictionary\n",
        "            prediction_dict_str = str(prediction).replace(\"'\", \"\\\"\")\n",
        "            print(f\"Model Prediction (Before Parsing): {prediction_dict_str}\")\n",
        "\n",
        "            model_predictions.append(prediction_dict_str.strip().lower())\n",
        "            ground_truth_str = str(example['category']).replace(\"'\", \"\\\"\")\n",
        "            print(f\"Ground Truth (Before Parsing): {ground_truth_str}\")\n",
        "\n",
        "            ground_truths.append(ground_truth_str.strip().lower())\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during model prediction: {e}\")\n",
        "\n",
        "    accuracy = compute_accuracy(gold_examples, model_predictions, ground_truths)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us now use this function to do one evaluation of all the two prompts assembled so far, each time computing the accuracy score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**(A) Evaluate zero shot prompt (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\"_________\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**(B) Evaluate few shot prompt (2 Marks)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "\"_________\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "However, this is just *one* choice of examples. We will need to run these evaluations with multiple choices of examples to get a sense of variability in F1 score for the few-shot prompt. As an example, let us run evaluations for the few-shot prompt 10 times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "num_eval_runs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "few_shot_performance = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for _ in tqdm(range(num_eval_runs)):\n",
        "\n",
        "    # For each run create a new sample of examples\n",
        "    examples = create_examples(laptop_reviews_df)\n",
        "\n",
        "    # Assemble the few shot prompt with these examples\n",
        "    few_shot_prompt = create_prompt(few_shot_system_message, examples, user_message_template)\n",
        "\n",
        "    # Evaluate prompt accuracy on gold examples\n",
        "    few_shot_accuracy = evaluate_prompt(few_shot_prompt, gold_examples, user_message_template)\n",
        "\n",
        "    few_shot_performance.append(few_shot_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**(C) Calculate Mean and Standard Deviation for Few Shot Prompt (4 Marks)**\n",
        "\n",
        "Compute the average (mean) and measure the variability (standard deviation) of the evaluation score for few shot prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"__________\"\n",
        "# Calculate for Few Shot Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**Step 5: Observation and Insights and Business perspective (3 Marks)**\n",
        "\n",
        "( Based on the projects, learner needs to share observations, learnings, insights and the business use case where these learnings can be beneficial.\n",
        "Provide a breakdown of the percentage of positive and negative reviews. Additionally, explain how this classification can assist InnovaTech Solutions in addressing the issues identified. )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "**-------------------------------------------------------------------------------------------------------End-------------------------------------------------------------------------------------------------------**"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
